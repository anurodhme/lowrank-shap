{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Low-Rank SHAP Prototype\n",
    "\n",
    "This notebook implements and tests the low-rank approximation for Kernel SHAP using SVD.\n",
    "\n",
    "**Goal**: Demonstrate ≤1.8 GB peak RAM for 10k×50 features using low-rank approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from lowrank_shap.lowrank_shap import LowRankSHAP, benchmark_comparison\n",
    "from lowrank_shap.baseline import KernelSHAPBaseline\n",
    "from lowrank_shap.data_utils import get_small_datasets\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data\n",
    "\n",
    "Use synthetic data for controlled experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_data(n_samples=1000, n_features=50, random_state=42):\n",
    "    \"\"\"Create synthetic data for testing.\"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Generate features\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    \n",
    "    # Create target with some non-linear relationship\n",
    "    weights = np.random.randn(n_features)\n",
    "    y = (X @ weights > 0).astype(int)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Test different sizes\n",
    "sizes = [1000, 2500, 5000, 7500, 10000]\n",
    "n_features = 50\n",
    "\n",
    "print(\"Creating synthetic datasets...\")\n",
    "datasets = {}\n",
    "for n in sizes:\n",
    "    X, y = create_synthetic_data(n_samples=n, n_features=n_features)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    datasets[n] = {\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'n_samples': n,\n",
    "        'n_features': n_features\n",
    "    }\n",
    "    print(f\"Created dataset: {n} samples, {n_features} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train Models on Synthetic Data\n",
    "\n",
    "Train simple models for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models on 1000 samples\n",
    "data_1k = datasets[1000]\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "rf.fit(data_1k['X_train'], data_1k['y_train'])\n",
    "\n",
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(data_1k['X_train'], data_1k['y_train'])\n",
    "\n",
    "print(\"Model performance on 1000 samples:\")\n",
    "print(f\"Random Forest: {rf.score(data_1k['X_test'], data_1k['y_test']):.3f}\")\n",
    "print(f\"Logistic Regression: {lr.score(data_1k['X_test'], data_1k['y_test']):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Memory Profiling\n",
    "\n",
    "Test memory usage for different dataset sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import os\n",
    "\n",
    "def profile_memory_usage(n_samples, n_features=50, rank=50):\n",
    "    \"\"\"Profile memory usage for low-rank SHAP.\"\"\"\n",
    "    \n",
    "    # Create data\n",
    "    X, y = create_synthetic_data(n_samples=n_samples, n_features=n_features)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    rf = RandomForestClassifier(n_estimators=20, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Use subset for background\n",
    "    background_size = min(100, len(X_train))\n",
    "    X_background = X_train[:background_size]\n",
    "    \n",
    "    # Use subset for testing\n",
    "    test_size = min(10, len(X_test))\n",
    "    X_test_small = X_test[:test_size]\n",
    "    \n",
    "    # Initialize and fit low-rank SHAP\n",
    "    explainer = LowRankSHAP(rank=rank)\n",
    "    \n",
    "    # Monitor memory\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_before = process.memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    explainer.fit(rf, X_background)\n",
    "    \n",
    "    memory_after_fit = process.memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    # Explain instances\n",
    "    shap_values, metadata = explainer.explain_dataset(X_test_small)\n",
    "    \n",
    "    memory_after_explain = process.memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    return {\n",
    "        'n_samples': n_samples,\n",
    "        'n_features': n_features,\n",
    "        'rank': rank,\n",
    "        'memory_before': memory_before,\n",
    "        'memory_after_fit': memory_after_fit,\n",
    "        'memory_after_explain': memory_after_explain,\n",
    "        'peak_memory': max(memory_after_fit, memory_after_explain),\n",
    "        'runtime': metadata['total_runtime'],\n",
    "        'kernel_matrix_size': n_samples * n_samples * 8 / 1024 / 1024  # MB\n",
    "    }\n",
    "\n",
    "# Test memory usage\n",
    "print(\"Profiling memory usage...\")\n",
    "memory_results = []\n",
    "\n",
    "for n in [1000, 2500, 5000, 7500, 10000]:\n",
    "    print(f\"Testing {n} samples...\")\n",
    "    result = profile_memory_usage(n, rank=50)\n",
    "    memory_results.append(result)\n",
    "    print(f\"Peak memory: {result['peak_memory']:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "memory_df = pd.DataFrame(memory_results)\n",
    "memory_df['memory_efficiency'] = memory_df['peak_memory'] / memory_df['kernel_matrix_size']\n",
    "memory_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Memory Usage Analysis\n",
    "\n",
    "Visualize memory usage vs dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Memory vs samples\n",
    "axes[0, 0].plot(memory_df['n_samples'], memory_df['peak_memory'], 'bo-', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Number of Samples')\n",
    "axes[0, 0].set_ylabel('Peak Memory (MB)')\n",
    "axes[0, 0].set_title('Memory Usage vs Dataset Size')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Memory efficiency\n",
    "axes[0, 1].plot(memory_df['n_samples'], memory_df['memory_efficiency'], 'ro-', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Number of Samples')\n",
    "axes[0, 1].set_ylabel('Memory Efficiency Ratio')\n",
    "axes[0, 1].set_title('Memory Efficiency vs Dataset Size')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Theoretical vs actual memory\n",
    "axes[1, 0].scatter(memory_df['kernel_matrix_size'], memory_df['peak_memory'], s=100)\n",
    "axes[1, 0].plot([0, max(memory_df['kernel_matrix_size'])], \n",
    "                [0, max(memory_df['kernel_matrix_size'])], 'r--', label='Theoretical')\n",
    "axes[1, 0].set_xlabel('Theoretical Kernel Matrix Size (MB)')\n",
    "axes[1, 0].set_ylabel('Actual Peak Memory (MB)')\n",
    "axes[1, 0].set_title('Theoretical vs Actual Memory Usage')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# 10k target line\n",
    "axes[1, 1].axhline(y=1800, color='r', linestyle='--', label='1.8 GB target')\n",
    "axes[1, 1].plot(memory_df['n_samples'], memory_df['peak_memory'], 'bo-', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Number of Samples')\n",
    "axes[1, 1].set_ylabel('Peak Memory (MB)')\n",
    "axes[1, 1].set_title('Memory Usage with 1.8 GB Target')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Accuracy vs Rank Analysis\n",
    "\n",
    "Test approximation accuracy for different ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy vs rank\n",
    "print(\"Testing accuracy vs rank...\")\n",
    "\n",
    "# Use 1000 samples for accuracy testing\n",
    "data_1k = datasets[1000]\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "rf.fit(data_1k['X_train'], data_1k['y_train'])\n",
    "\n",
    "# Use small subset for comparison\n",
    "background_size = min(50, len(data_1k['X_train']))\n",
    "X_background = data_1k['X_train'][:background_size]\n",
    "X_test_small = data_1k['X_test'][:5]\n",
    "\n",
    "# Run benchmark comparison\n",
    "ranks_to_test = [5, 10, 20, 30, 50]\n",
    "benchmark_results = benchmark_comparison(\n",
    "    rf, X_background, X_test_small, ranks=ranks_to_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze accuracy results\n",
    "accuracy_data = []\n",
    "\n",
    "exact_shap = benchmark_results['exact']['shap_values']\n",
    "\n",
    "for rank, results in benchmark_results['low_rank'].items():\n",
    "    accuracy_data.append({\n",
    "        'rank': rank,\n",
    "        'mean_relative_error': results['mean_relative_error'],\n",
    "        'max_relative_error': np.max(results['relative_error']),\n",
    "        'runtime': results['metadata']['total_runtime'],\n",
    "        'memory': results['metadata']['max_memory']\n",
    "    })\n",
    "\n",
    "accuracy_df = pd.DataFrame(accuracy_data)\n",
    "accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy vs rank\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Relative error vs rank\n",
    "axes[0, 0].plot(accuracy_df['rank'], accuracy_df['mean_relative_error'], 'bo-', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Rank (k)')\n",
    "axes[0, 0].set_ylabel('Mean Relative Error')\n",
    "axes[0, 0].set_title('Accuracy vs Rank')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Runtime vs rank\n",
    "axes[0, 1].plot(accuracy_df['rank'], accuracy_df['runtime'], 'ro-', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Rank (k)')\n",
    "axes[0, 1].set_ylabel('Runtime (s)')\n",
    "axes[0, 1].set_title('Runtime vs Rank')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Memory vs rank\n",
    "axes[1, 0].plot(accuracy_df['rank'], accuracy_df['memory'], 'go-', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Rank (k)')\n",
    "axes[1, 0].set_ylabel('Memory (MB)')\n",
    "axes[1, 0].set_title('Memory vs Rank')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Error distribution\n",
    "errors = []\n",
    "for rank, results in benchmark_results['low_rank'].items():\n",
    "    errors.extend(results['relative_error'])\n",
    "\n",
    "axes[1, 1].boxplot([benchmark_results['low_rank'][r]['relative_error'] for r in ranks_to_test],\n",
    "                   labels=ranks_to_test)\n",
    "axes[1, 1].set_xlabel('Rank (k)')\n",
    "axes[1, 1].set_ylabel('Relative Error')\n",
    "axes[1, 1].set_title('Error Distribution by Rank')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Week 2 Summary\n",
    "\n",
    "Summary of low-rank SHAP prototype results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary\n",
    "summary = {\n",
    "    'memory_target_met': memory_df['peak_memory'].max() < 1800,\n",
    "    'max_memory_mb': memory_df['peak_memory'].max(),\n",
    "    'best_accuracy_rank': accuracy_df.loc[accuracy_df['mean_relative_error'].idxmin(), 'rank'],\n",
    "    'best_accuracy_error': accuracy_df['mean_relative_error'].min(),\n",
    "    'memory_savings': memory_df['kernel_matrix_size'].iloc[-1] / memory_df['peak_memory'].iloc[-1],\n",
    "    'speedup': benchmark_results['exact']['metadata']['total_runtime'] / \
",
    "               accuracy_df.loc[accuracy_df['mean_relative_error'].idxmin(), 'runtime']\n",
    "}\n",
    "\n",
    "print(\"=== WEEK 2 PROTOTYPE SUMMARY ===\")\n",
    "print(f\"Memory target (≤1.8 GB): {summary['memory_target_met']} ({summary['max_memory_mb']:.1f} MB max)\")\n",
    "print(f\"Best accuracy at rank {summary['best_accuracy_rank']}: {summary['best_accuracy_error']:.3f} relative error\")\n",
    "print(f\"Memory savings: {summary['memory_savings']:.1f}x vs exact\")\n",
    "print(f\"Speedup: {summary['speedup']:.1f}x vs exact\")\n",
    "\n",
    "# Save results\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "memory_df.to_csv('../results/memory_benchmark.csv', index=False)\n",
    "accuracy_df.to_csv('../results/accuracy_benchmark.csv', index=False)\n",
    "\n",
    "import json\n",
    "with open('../results/week2_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\n✅ Week 2 Task Complete: Low-rank SHAP prototype validated\")\n",
    "print(\"Results saved to ../results/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
