{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Kernel SHAP Baseline Testing\n",
    "\n",
    "This notebook reproduces standard Kernel SHAP on small tabular datasets (â‰¤10k rows) to establish baseline RAM/time measurements.\n",
    "\n",
    "**Goal**: Measure memory usage and runtime for exact Kernel SHAP as baseline for low-rank approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from lowrank_shap.baseline import KernelSHAPBaseline, benchmark_kernel_shap\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Datasets\n",
    "\n",
    "We'll use the smallest datasets first to establish baseline measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "data_path = '../data/raw'\n",
    "\n",
    "# Wine Quality (smallest dataset)\n",
    "wine_df = pd.read_csv(os.path.join(data_path, 'wine.csv'))\n",
    "print(f\"Wine dataset: {wine_df.shape[0]} rows, {wine_df.shape[1]} columns\")\n",
    "\n",
    "# Bike Sharing\n",
    "bike_df = pd.read_csv(os.path.join(data_path, 'bike.csv'))\n",
    "print(f\"Bike dataset: {bike_df.shape[0]} rows, {bike_df.shape[1]} columns\")\n",
    "\n",
    "# Show first few rows\n",
    "wine_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Wine Quality Dataset\n",
    "\n",
    "Target: quality (classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare wine dataset\n",
    "X_wine = wine_df.drop('quality', axis=1).values\n",
    "y_wine = wine_df['quality'].values\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_wine_scaled = scaler.fit_transform(X_wine)\n",
    "\n",
    "# Split data\n",
    "X_train_wine, X_test_wine, y_train_wine, y_test_wine = train_test_split(\n",
    "    X_wine_scaled, y_wine, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Wine - Train: {X_train_wine.shape}, Test: {X_test_wine.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Models\n",
    "\n",
    "Train simple models for testing Kernel SHAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest on Wine\n",
    "rf_wine = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_wine.fit(X_train_wine, y_train_wine)\n",
    "\n",
    "# Train Logistic Regression on Wine\n",
    "lr_wine = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_wine.fit(X_train_wine, y_train_wine)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Wine Dataset Performance:\")\n",
    "print(f\"Random Forest: {accuracy_score(y_test_wine, rf_wine.predict(X_test_wine)):.3f}\")\n",
    "print(f\"Logistic Regression: {accuracy_score(y_test_wine, lr_wine.predict(X_test_wine)):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Kernel SHAP Baseline Testing\n",
    "\n",
    "Test exact Kernel SHAP on small subsets to establish baseline measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on small subset (100 instances)\n",
    "n_test = min(100, len(X_test_wine))\n",
    "X_test_small = X_test_wine[:n_test]\n",
    "\n",
    "print(f\"Testing Kernel SHAP on {n_test} instances...\")\n",
    "print(f\"Features: {X_test_small.shape[1]}\")\n",
    "\n",
    "# Use training data as background\n",
    "background_size = min(100, len(X_train_wine))\n",
    "X_background = X_train_wine[:background_size]\n",
    "\n",
    "print(f\"Background samples: {background_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with Random Forest\n",
    "print(\"=== Random Forest Kernel SHAP Baseline ===\")\n",
    "\n",
    "# Test different sample sizes\n",
    "sample_sizes = [512, 1024, 2048]\n",
    "results = []\n",
    "\n",
    "for n_samples in sample_sizes:\n",
    "    print(f\"\\nTesting with {n_samples} samples...\")\n",
    "    \n",
    "    benchmark_result = benchmark_kernel_shap(\n",
    "        rf_wine, X_background, X_test_small[:5], n_samples=n_samples\n",
    "    )\n",
    "    \n",
    "    result = {\n",
    "        'model': 'RandomForest',\n",
    "        'n_samples': n_samples,\n",
    "        'runtime': benchmark_result['metadata']['total_runtime'],\n",
    "        'memory_mb': benchmark_result['metadata']['max_memory'],\n",
    "        'instances': benchmark_result['metadata']['total_instances']\n",
    "    }\n",
    "    \n",
    "    results.append(result)\n",
    "    print(f\"Runtime: {result['runtime']:.2f}s, Memory: {result['memory_mb']:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['runtime_per_instance'] = results_df['runtime'] / results_df['instances']\n",
    "results_df['memory_per_instance'] = results_df['memory_mb'] / results_df['instances']\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Memory and Runtime Analysis\n",
    "\n",
    "Establish baseline measurements for exact Kernel SHAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Runtime vs samples\n",
    "axes[0, 0].plot(results_df['n_samples'], results_df['runtime'], 'bo-')\n",
    "axes[0, 0].set_xlabel('Number of Samples')\n",
    "axes[0, 0].set_ylabel('Total Runtime (s)')\n",
    "axes[0, 0].set_title('Runtime vs Sample Size')\n",
    "\n",
    "# Memory vs samples\n",
    "axes[0, 1].plot(results_df['n_samples'], results_df['memory_mb'], 'ro-')\n",
    "axes[0, 1].set_xlabel('Number of Samples')\n",
    "axes[0, 1].set_ylabel('Peak Memory (MB)')\n",
    "axes[0, 1].set_title('Memory Usage vs Sample Size')\n",
    "\n",
    "# Runtime per instance\n",
    "axes[1, 0].plot(results_df['n_samples'], results_df['runtime_per_instance'], 'go-')\n",
    "axes[1, 0].set_xlabel('Number of Samples')\n",
    "axes[1, 0].set_ylabel('Runtime per Instance (s)')\n",
    "axes[1, 0].set_title('Runtime Efficiency')\n",
    "\n",
    "# Memory per instance\n",
    "axes[1, 1].plot(results_df['n_samples'], results_df['memory_per_instance'], 'mo-')\n",
    "axes[1, 1].set_xlabel('Number of Samples')\n",
    "axes[1, 1].set_ylabel('Memory per Instance (MB)')\n",
    "axes[1, 1].set_title('Memory Efficiency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Baseline Summary\n",
    "\n",
    "Key findings for exact Kernel SHAP baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate baseline metrics\n",
    "baseline_summary = {\n",
    "    'dataset': 'Wine Quality',\n",
    "    'n_features': X_wine.shape[1],\n",
    "    'n_instances': len(X_test_small),\n",
    "    'avg_runtime_per_instance': results_df['runtime_per_instance'].mean(),\n",
    "    'avg_memory_per_instance': results_df['memory_per_instance'].mean(),\n",
    "    'projected_1k_instances': {\n",
    "        'runtime_minutes': results_df['runtime_per_instance'].mean() * 1000 / 60,\n",
    "        'memory_gb': results_df['memory_per_instance'].mean() * 1000 / 1024\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=== KERNEL SHAP BASELINE SUMMARY ===\")\n",
    "print(f\"Dataset: {baseline_summary['dataset']}\")\n",
    "print(f\"Features: {baseline_summary['n_features']}\")\n",
    "print(f\"Test instances: {baseline_summary['n_instances']}\")\n",
    "print(f\"Avg runtime per instance: {baseline_summary['avg_runtime_per_instance']:.2f}s\")\n",
    "print(f\"Avg memory per instance: {baseline_summary['avg_memory_per_instance']:.1f} MB\")\n",
    "print(f\"Projected for 1k instances: {baseline_summary['projected_1k_instances']['runtime_minutes']:.1f} min, {baseline_summary['projected_1k_instances']['memory_gb']:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Baseline Results\n",
    "\n",
    "Save baseline measurements for comparison with low-rank approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "# Save detailed results\n",
    "results_df.to_csv('../results/baseline_kernel_shap_results.csv', index=False)\n",
    "\n",
    "# Save summary\n",
    "import json\n",
    "with open('../results/baseline_summary.json', 'w') as f:\n",
    "    json.dump(baseline_summary, f, indent=2)\n",
    "\n",
    "print(\"Baseline results saved to ../results/\")\n",
    "print(\"âœ… Week 1 Task Complete: Kernel SHAP baseline established\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
